{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which I use extremely randomized trees. See here:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "\n",
    "Also see: \n",
    "[GEW2006] P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time\n",
    "#load data:\n",
    "# train = pd.read_csv(\"train.csv\")\n",
    "# target = train['target']\n",
    "# #drop targets & (unique row) IDs from training data\n",
    "# train = train.drop(['ID','target'],axis=1)\n",
    "# test = pd.read_csv(\"test.csv\")\n",
    "# IDs = test['ID'].values\n",
    "# test = test.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146/code\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = train_series.mean()  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19133/feature-engineering-for-beginners\n",
    "#check this out: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19240/analysis-of-duplicate-variables-correlated-variables-large-post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to report best hyperparameters:\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING EXTRATREES CLASSIFIER APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/yuhaichina/bnp-paribas-cardif-claims-management/extratreesclassifier-score-0-45911\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "t0 = time.time()\n",
    "X_train = train\n",
    "X_test = test\n",
    "extc = ExtraTreesClassifier(n_estimators=700,max_features= 50,criterion= 'entropy',min_samples_split= 5,\n",
    "                            max_depth= 50, min_samples_leaf= 5)      \n",
    "\n",
    "extc.fit(X_train,target) \n",
    "preds = extc.predict_proba(X_test)[:,1]\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions_file = open(\"extc.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(IDs, preds[:,1]))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV on ETC parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "t0 = time.time()\n",
    "X_train = train\n",
    "X_test = test\n",
    "\n",
    "param_grid = {'max_depth': range(20,50),\n",
    "                      'n_estimators': np.arange(500,1000,100),\n",
    "#                         'n_estimators': [10],\n",
    "                      'max_features' : np.arange(10,110,10),\n",
    "                      'min_samples_split' : np.arange(1,6,1),\n",
    "                        'min_samples_leaf' : np.arange(1,6,1),\n",
    "                      'criterion' : ['entropy'],\n",
    "                      #'scale_pos_weight': [0.5, 1]\n",
    "                      #'model__eta':[0.01,0.02],\n",
    "                     #'model__scale_pos_weight':[0.8,1.0]\n",
    "                      #'model__silent':[1],\n",
    "                      }\n",
    "\n",
    "\n",
    "\n",
    "extc = ExtraTreesClassifier()\n",
    "\n",
    "n_iter_search=50\n",
    "random_search = RandomizedSearchCV(extc, param_distributions=param_grid,\n",
    "                                   n_iter=n_iter_search, scoring =\"log_loss\")\n",
    "\n",
    "random_search.fit( train , target)\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'min_samples_leaf': 2, \n",
    "         'n_estimators': 800, \n",
    "         'max_features': 50, \n",
    "         'criterion': 'entropy', \n",
    "         'min_samples_split': 2, \n",
    "         'max_depth': 29}\n",
    "t0 = time.time()\n",
    "extc2 = ExtraTreesClassifier(**params)\n",
    "#OR\n",
    "#extc2 = ExtraTreesClassifier(**random_search.best_params_)\n",
    "extc2.fit(X_train,target) \n",
    "preds_2 = extc.predict_proba(X_test)[:,1]\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions_file = open(\"extc2_rcv.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(ids, preds))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave -logloss = 0.45450."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_import = extc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.shape(feat_import)\n",
    "print feat_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO THE SAME NOW, INCLUDING A BIT MORE PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation on extc benchmark. See here: \n",
    "https://www.kaggle.com/mujtabaasif/bnp-paribas-cardif-claims-management/extratrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the benchmark:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target'].values\n",
    "train = train.drop(['ID','target','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -999 \n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -999\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n",
    "print('Training...')\n",
    "extc = ExtraTreesClassifier(n_estimators=850,max_features= 60,criterion= 'entropy',min_samples_split= 4,\n",
    "                            max_depth= 40, min_samples_leaf= 2, n_jobs = -1)      \n",
    "\n",
    "extc.fit(X_train,target) \n",
    "\n",
    "print('Predict...')\n",
    "y_pred = extc.predict_proba(X_test)\n",
    "#print y_pred\n",
    "\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred[:,1]}).to_csv('extra_trees.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target'].values\n",
    "train = train.drop(['ID','target','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = train_series.mean()\n",
    "            \n",
    "X_train = train\n",
    "X_test = test\n",
    "print('Training...')\n",
    "extc = ExtraTreesClassifier(n_estimators=850,max_features= 60,criterion= 'entropy',min_samples_split= 4,\n",
    "                            max_depth= 40, min_samples_leaf= 2, n_jobs = -1)      \n",
    "\n",
    "extc.fit(X_train,target) \n",
    "\n",
    "print('Predict...')\n",
    "y_pred = extc.predict_proba(X_test)\n",
    "#print y_pred\n",
    "\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred[:,1]}).to_csv('extra_trees_impute.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK now, using the preprocessing above, let's use RandomizedSearchCV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Cleaning...\n",
      "Training...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugobowne-anderson/repos/scikit-learn/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/hugobowne-anderson/repos/scikit-learn/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "import time\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target'].values\n",
    "train = train.drop(['ID','target','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID','v8','v23','v25','v31','v36','v37','v46','v51','v53','v54','v63','v73','v75','v79','v81','v82','v89','v92','v95','v105','v107','v108','v109','v110','v116','v117','v118','v119','v123','v124','v128'],axis=1)\n",
    "\n",
    "print('Cleaning...')\n",
    "\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = train_series.mean()\n",
    "X_train = train\n",
    "X_test = test\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "param_grid = {'n_estimators': [850],\n",
    "#                         'n_estimators': [10],\n",
    "                      'max_features' : np.arange(50,90,5),\n",
    "                      'min_samples_split' : np.arange(2,5,1),\n",
    "                        'min_samples_leaf' : np.arange(1,4,1),\n",
    "                      'criterion' : ['entropy'],\n",
    "                      #'scale_pos_weight': [0.5, 1]\n",
    "                      #'model__eta':[0.01,0.02],\n",
    "                     #'model__scale_pos_weight':[0.8,1.0]\n",
    "                      #'model__silent':[1],\n",
    "                      }\n",
    "\n",
    "\n",
    "\n",
    "extc = ExtraTreesClassifier(verbose = 10 )\n",
    "\n",
    "n_iter_search= 15\n",
    "random_search = RandomizedSearchCV(extc, param_distributions=param_grid,\n",
    "                                   n_iter=n_iter_search, scoring =\"log_loss\" , n_jobs = -1,\n",
    "                                  verbose = 10)\n",
    "\n",
    "random_search.fit( X_train , target)\n",
    "\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/scirpus/bnp-paribas-cardif-claims-management/benouilli-naive-bayes/code\n",
    "#https://www.kaggle.com/chabir/bnp-paribas-cardif-claims-management/extratreesclassifier-score-0-45-v5/discussion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
